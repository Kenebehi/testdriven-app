---

  name: 'Sync S3 DAGs and settings'
  
  on:
    release:
      types:
        - published
  
  jobs:
    sync-dags:
      name: Copy DAGs to S3
      runs-on: ubuntu-latest
      steps:
        - name: Checkout repo
          uses: actions/checkout@v3
          with:
            fetch-depth: 2

        - name: Get Previous Commit SHA
          run: |
              PREV_SHA=$(git rev-parse HEAD~1)
              echo "Previous SHA: $PREV_SHA"
  
        - name: Get changed Airflow  DAG's
          id: updated-dags-files
          uses: tj-actions/changed-files@v35
          with:
            base_sha: "${{ github.event.workflow_run.head_sha }}"
            files: |
              airflow/dags/**
  

  
        - name: S3 Sync DAGs
          if: ${{ steps.updated-dags-files.outputs.all_changed_and_modified_files !=0 }}
          env:
            S3_AIRFLOW_DIR: s3://product-analytics-pipelines/airflow
          run: |
            
            updated_dag_files="${{ steps.updated-dags-files.outputs.all_changed_and_modified_files }}"
            include=""
            for file in $updated_dag_files; do
              include="$include --include $(basename $file)"
            done
  
            echo "${{ steps.updated-dags-files.outputs.all_changed_and_modified_files }}"
            
            aws s3 sync --delete ./airflow/dags/ $S3_AIRFLOW_DIR/staging/dags/ --exclude "*"  $include;
            aws s3 sync --delete ./airflow/dags/ $S3_AIRFLOW_DIR/live/dags/ --exclude "*"  $include;
            aws s3 sync --delete ./airflow/dags/ $S3_AIRFLOW_DIR/${{ github.event.release.tag_name }}/dags/ --exclude "*"  $include;
